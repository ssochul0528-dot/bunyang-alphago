import asyncio
import httpx
import csv
import os
import random
from sqlmodel import Session, select
from main import engine, Site, create_db_and_tables

async def sync_and_save_to_csv():
    print("ğŸš€ Starting Global Site Sync & CSV Export (Weekly Update Mode)")
    create_db_and_tables()
    
    keywords = [
        "ë˜ë¯¸ì•ˆ", "íìŠ¤í…Œì´íŠ¸", "í‘¸ë¥´ì§€ì˜¤", "eí¸í•œì„¸ìƒ", "ìì´", "ë”ìƒµ", "ë¡¯ë°ìºìŠ¬", "SKë·°", "ì•„ì´íŒŒí¬",
        "í¬ë ˆë‚˜", "í˜¸ë°˜", "ë°ì‹œì•™", "í•˜ëŠ˜ì±„", "ìŠ¤ìœ„ì²¸", "ë¦¬ìŠˆë¹Œ", "ë”í”Œë˜í‹°ë„˜", "ì„¼íŠ¸ë ˆë¹Œ", "ë¹„ë°œë””", "ê¸ˆí˜¸ì–´ìš¸ë¦¼", 
        "ì œì¼í’ê²½ì±„", "ì¤‘í¥", "ë°˜ë„ìœ ë³´ë¼", "ë””ì—íŠ¸ë¥´", "ìš°ë¯¸ë¦°", "ë‘ì‚°ìœ„ë¸Œ", "ë¼ì¸ê±´ì„¤", "ì–‘ìš°ë‚´ì•ˆì• ", 
        "ì„œí¬ìŠ¤íƒ€íìŠ¤", "í•œì‹ ë”íœ´", "ë™ë¬¸êµ¿ëª¨ë‹í", "ì´ìˆ˜ê±´ì„¤", "í•œë¦¼í’€ì—ë²„", "ë™ì¼í”Œë¼ì›Œ", "ë¼ì˜¨í”„ë¼ì´ë¹—", 
        "ì´ì§€ë”ì›", "ì‚¼ì •ê·¸ë¦°ì½”ì•„", "ìœ ë³´ë¼", "ë¯¼ê°„ì„ëŒ€", "ê³µê³µì§€ì›", "ë¶„ì–‘ì¤‘", "ë¶„ì–‘ì˜ˆì •", "ì„ ì°©ìˆœ", 
        "ë¯¸ë¶„ì–‘", "ì”ì—¬ì„¸ëŒ€", "ë°œê¸°ì¸ëª¨ì§‘", "ì§€ì—­ì£¼íƒì¡°í•©", "ì§€ì£¼íƒ", "í•´ë§í„´", "ì¨ë°‹", "ë””ì—íŠ¸ë¥´", 
        "ì´ì•ˆ", "ì—˜ë¦¬ì›€", "íŒŒë¼ê³¤", "ì•„ë„ˆìŠ¤ë¹Œ", "ìˆ˜ìì¸", "ë² ë¥´ë””ì›€"
    ]
    
    new_count = 0
    async with httpx.AsyncClient(follow_redirects=True) as client:
        for kw in keywords:
            try:
                print(f"Scanning: {kw}...", end=" ", flush=True)
                fake_nnb = "".join(random.choices("0123456789ABCDEF", k=16))
                h = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                    "Cookie": f"NNB={fake_nnb}"
                }
                url = "https://isale.land.naver.com/iSale/api/complex/searchList"
                params = {
                    "keyword": kw, 
                    "complexType": "APT:ABYG:JGC:OR:OP:VL:DDD:ABC:ETC:UR:HO:SH", 
                    "salesStatus": "0:1:2:3:4:5:6:7:8:9:10:11:12", 
                    "pageSize": "100"
                }
                res = await client.get(url, params=params, headers=h, timeout=12.0)
                
                if res.status_code == 200:
                    data = res.json()
                    items = data.get("result", {}).get("list", [])
                    with Session(engine) as session:
                        added = 0
                        for it in items:
                            sid = f"extern_isale_{it.get('complexNo')}"
                            if not session.get(Site, sid):
                                session.add(Site(
                                    id=sid, 
                                    name=it.get("complexName"), 
                                    address=it.get("address"),
                                    brand=it.get("h_name"), 
                                    category=it.get("complexTypeName", "ë¶€ë™ì‚°"),
                                    price=1900.0, 
                                    target_price=2200.0, 
                                    supply=500, 
                                    status=it.get("salesStatusName")
                                ))
                                new_count += 1
                                added += 1
                        session.commit()
                        print(f"Found {len(items)} items, Added {added} new.")
                else:
                    print(f"Error {res.status_code}")
                
                await asyncio.sleep(1.5) # Anti-blocking delay
            except Exception as e:
                print(f"Failed: {e}")

    # Export all sites to CSV
    print("\nğŸ“ Exporting all data to sites_data.csv...")
    with Session(engine) as session:
        all_sites = session.exec(select(Site)).all()
        
        with open("sites_data.csv", "w", encoding="utf-8", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["id", "name", "address", "brand", "category", "price", "target_price", "supply", "down_payment", "interest_benefit", "status"])
            for s in all_sites:
                writer.writerow([
                    s.id, s.name, s.address, s.brand, s.category, 
                    s.price, s.target_price, s.supply, 
                    s.down_payment or "10%", s.interest_benefit or "ë¬´ì´ì", 
                    s.status
                ])
    
    print(f"âœ… Sync Complete. Total {len(all_sites)} sites saved to CSV.")

if __name__ == "__main__":
    asyncio.run(sync_and_save_to_csv())
