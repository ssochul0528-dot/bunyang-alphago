import asyncio
import httpx
import csv
import os
import random
import time
from sqlmodel import Session, select
from main import engine, Site, create_db_and_tables

async def sync_all_industrial():
    print("ğŸš€ Starting INDUSTRIAL Full-Coverage Sync (200+ Regional Scans)")
    create_db_and_tables()
    
    # 1. More granular Regional Keywords (Si/Gun/Gu)
    seoul = ["ê°•ë‚¨êµ¬", "ê°•ë™êµ¬", "ê°•ë¶êµ¬", "ê°•ì„œêµ¬", "ê´€ì•…êµ¬", "ê´‘ì§„êµ¬", "êµ¬ë¡œêµ¬", "ê¸ˆì²œêµ¬", "ë…¸ì›êµ¬", "ë„ë´‰êµ¬", "ë™ëŒ€ë¬¸êµ¬", "ë™ì‘êµ¬", "ë§ˆí¬êµ¬", "ì„œëŒ€ë¬¸êµ¬", "ì„œì´ˆêµ¬", "ì„±ë™êµ¬", "ì„±ë¶êµ¬", "ì†¡íŒŒêµ¬", "ì–‘ì²œêµ¬", "ì˜ë“±í¬êµ¬", "ìš©ì‚°êµ¬", "ì€í‰êµ¬", "ì¢…ë¡œêµ¬", "ì¤‘êµ¬", "ì¤‘ë‘êµ¬"]
    gyeonggi = ["ìˆ˜ì›ì‹œ", "ì„±ë‚¨ì‹œ", "ì˜ì •ë¶€ì‹œ", "ì•ˆì–‘ì‹œ", "ë¶€ì²œì‹œ", "ê´‘ëª…ì‹œ", "í‰íƒì‹œ", "ë™ë‘ì²œì‹œ", "ì•ˆì‚°ì‹œ", "ê³ ì–‘ì‹œ", "ê³¼ì²œì‹œ", "êµ¬ë¦¬ì‹œ", "ë‚¨ì–‘ì£¼ì‹œ", "ì˜¤ì‚°ì‹œ", "ì‹œí¥ì‹œ", "êµ°í¬ì‹œ", "ì˜ì™•ì‹œ", "í•˜ë‚¨ì‹œ", "ìš©ì¸ì‹œ", "íŒŒì£¼ì‹œ", "ì´ì²œì‹œ", "ì•ˆì„±ì‹œ", "ê¹€í¬ì‹œ", "í™”ì„±ì‹œ", "ê´‘ì£¼ì‹œ", "ì–‘ì£¼ì‹œ", "í¬ì²œì‹œ", "ì—¬ì£¼ì‹œ"]
    incheon = ["ë¯¸ì¶”í™€êµ¬", "ì—°ìˆ˜êµ¬", "ë‚¨ë™êµ¬", "ë¶€í‰êµ¬", "ê³„ì–‘êµ¬", "ì¸ì²œ ì„œêµ¬", "ì˜ì¢…ë„"]
    busan = ["ë¶€ì‚°ì§„êµ¬", "ë™ë˜êµ¬", "í•´ìš´ëŒ€êµ¬", "ì‚¬í•˜êµ¬", "ê°•ì„œêµ¬", "ì—°ì œêµ¬", "ìˆ˜ì˜êµ¬", "ê¸°ì¥êµ°"]
    other_major = ["ì²œì•ˆ", "ì²­ì£¼", "ì „ì£¼", "ì°½ì›", "í¬í•­", "êµ¬ë¯¸", "ê¹€í•´", "ìˆœì²œ", "ì—¬ìˆ˜", "ì›ì£¼", "ì¶˜ì²œ", "ì œì£¼", "ì„¸ì¢…"]
    
    marketing = ["ë¶„ì–‘ì¤‘", "ë¶„ì–‘ì˜ˆì •", "ë¯¸ë¶„ì–‘", "ì„ ì°©ìˆœ", "ì”ì—¬ì„¸ëŒ€", "ë¯¼ê°„ì„ëŒ€"]

    keywords = sorted(list(set(seoul + gyeonggi + incheon + busan + other_major + marketing)))
    
    # 2. Random User-Agents
    uas = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Edge/120.0.0.0",
        "Mozilla/5.0 (iPhone; CPU iPhone OS 17_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1"
    ]
    
    print(f"Plan: {len(keywords)} keywords to scan.")
    
    new_count = 0
    total_found = 0
    
    async with httpx.AsyncClient(follow_redirects=False) as client:
        for i, kw in enumerate(keywords):
            try:
                print(f"[{i+1}/{len(keywords)}] {kw}:", end=" ", flush=True)
                
                # Randomized Delay to mimic human
                await asyncio.sleep(random.uniform(1.5, 3.5)) 
                
                fake_nnb = "".join(random.choices("0123456789abcdef", k=16))
                ua = random.choice(uas)
                h = {
                    "User-Agent": ua,
                    "Cookie": f"NNB={fake_nnb}",
                    "Referer": "https://isale.land.naver.com/"
                }
                
                url = "https://isale.land.naver.com/iSale/api/complex/searchList"
                params = {
                    "keyword": kw, 
                    "complexType": "APT:ABYG:JGC:OR:OP:VL:DDD:ABC:ETC:UR:HO:SH", 
                    "salesStatus": "0:1:2:3:4:5:6:7:8:9:10:11:12", 
                    "pageSize": "100"
                }
                
                res = await client.get(url, params=params, headers=h, timeout=10.0)
                
                if res.status_code == 200:
                    data = res.json()
                    items = data.get("result", {}).get("list", [])
                    total_found += len(items)
                    
                    with Session(engine) as session:
                        added = 0
                        for it in items:
                            sid = f"extern_isale_{it.get('complexNo')}"
                            if not session.get(Site, sid):
                                session.add(Site(
                                    id=sid, 
                                    name=it.get("complexName"), 
                                    address=it.get("address"),
                                    brand=it.get("h_name"), 
                                    category=it.get("complexTypeName", "ë¶€ë™ì‚°"),
                                    price=1900.0, target_price=2200.0, supply=500, 
                                    status=it.get("salesStatusName")
                                ))
                                new_count += 1
                                added += 1
                        session.commit()
                        print(f"{len(items)} items ({added} new).")
                elif res.status_code == 302:
                    print("Blocked (302).")
                    await asyncio.sleep(10) # Heavy sleep if blocked
                else:
                    print(f"Error {res.status_code}.")
                
            except Exception as e:
                print(f"Fail: {e}")

    # Export all sites to CSV
    print(f"\nğŸ“ Exporting {total_found} cumulative items to CSV...")
    with Session(engine) as session:
        all_sites = session.exec(select(Site)).all()
        with open("sites_data.csv", "w", encoding="utf-8", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["id", "name", "address", "brand", "category", "price", "target_price", "supply", "down_payment", "interest_benefit", "status"])
            for s in all_sites:
                writer.writerow([s.id, s.name, s.address, s.brand, s.category, s.price, s.target_price, s.supply, s.down_payment or "10%", s.interest_benefit or "ë¬´ì´ì", s.status])
    
    print(f"âœ… Industrial Sync Complete. Total {len(all_sites)} unique sites in DB.")

if __name__ == "__main__":
    asyncio.run(sync_all_industrial())
