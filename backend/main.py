from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import random
import datetime
import os
import uvicorn
import asyncio
from contextlib import asynccontextmanager
from sqlmodel import Field, Session, SQLModel, create_engine, select, or_, col
import logging
import httpx
import google.generativeai as genai
import json
from typing import List, Optional, Union, Any

# Gemini API ì„¤ì •
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "AIzaSyCd5wNhgfAFZWpHdGDA9RSzpQ-YZeTHms0")
genai.configure(api_key=GEMINI_API_KEY)

import logging
import re

# êµ¬ê¸€ ì‹œíŠ¸ ì›¹í›… URL (ì‚¬ìš©ìê°€ ì„¤ì •í•œ URL)
GOOGLE_SHEET_WEBHOOK_URL = "https://script.google.com/macros/s/AKfycbzZLa5HVuEdHpoD3ip6908XGyagJFsfsfJAmlfxLOekrqad0625QbYV4TLai4xHswwDfw/exec"
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def extract_json(text: str):
    """ë¬¸ìì—´ì—ì„œ JSON ë¸”ë¡ë§Œ ì¶”ì¶œí•˜ëŠ” ê³ ë„í™”ëœ í•¨ìˆ˜ (RegEx ì‚¬ìš©)"""
    if not text:
        return None
    
    # 1. ```json ë¸”ë¡ ì¶”ì¶œ ì‹œë„
    match = re.search(r"```json\s*(\{.*?\})\s*```", text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1).strip())
        except: pass
        
    # 2. ì¼ë°˜ ``` ë¸”ë¡ ì¶”ì¶œ ì‹œë„
    match = re.search(r"```\s*(\{.*?\})\s*```", text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1).strip())
        except: pass

    # 3. í…ìŠ¤íŠ¸ ë‚´ì˜ ì²« ë²ˆì§¸ { ì™€ ë§ˆì§€ë§‰ } ì‚¬ì´ ì¶”ì¶œ ì‹œë„
    match = re.search(r"(\{.*\})", text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1).strip())
        except: pass
        
    # 4. ì „ì²´ í…ìŠ¤íŠ¸ ì‹œë„
    try:
        return json.loads(text.strip())
    except:
        logger.error(f"Failed to parse AI JSON response: {text[:200]}...")
        return None

# --- Database Setup ---
sqlite_file_name = "database.db"
sqlite_url = f"sqlite:///{sqlite_file_name}"
engine = create_engine(sqlite_url, connect_args={"check_same_thread": False})

class Site(SQLModel, table=True):
    __table_args__ = {'extend_existing': True}
    
    id: str = Field(primary_key=True)
    name: str
    address: str
    brand: Optional[str] = None
    category: str
    price: float
    target_price: float
    supply: int
    down_payment: Optional[str] = "10%"
    interest_benefit: Optional[str] = "ì¤‘ë„ê¸ˆ ë¬´ì´ì"
    status: Optional[str] = None
    last_updated: datetime.datetime = Field(default_factory=datetime.datetime.now)

class Lead(SQLModel, table=True):
    __table_args__ = {'extend_existing': True}
    id: Optional[int] = Field(default=None, primary_key=True)
    name: str
    phone: str
    rank: str
    site: str
    source: Optional[str] = Field(default="ì•Œ ìˆ˜ ì—†ìŒ")
    created_at: datetime.datetime = Field(default_factory=datetime.datetime.now)

# --- NATIONWIDE START DATA ---
MOCK_SITES = [
    {"id": "seoul_seocho_1", "name": "ë©”ì´í”Œìì´", "address": "ì„œìš¸íŠ¹ë³„ì‹œ ì„œì´ˆêµ¬ ì ì›ë™", "brand": "ìì´", "category": "ì•„íŒŒíŠ¸", "price": 6700, "target_price": 7500, "supply": 3307, "status": "ë¶„ì–‘ì¤‘"},
    {"id": "seoul_seocho_2", "name": "ë˜ë¯¸ì•ˆ ì›íœíƒ€ìŠ¤", "address": "ì„œìš¸íŠ¹ë³„ì‹œ ì„œì´ˆêµ¬ ë°˜í¬ë™", "brand": "ë˜ë¯¸ì•ˆ", "category": "ì•„íŒŒíŠ¸", "price": 6800, "target_price": 7800, "supply": 641, "status": "ë¶„ì–‘ì¤‘"},
    {"id": "seoul_gangnam_1", "name": "ì²­ë‹´ ë¥´ì—˜", "address": "ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ ì²­ë‹´ë™", "brand": "ë¥´ì—˜", "category": "ì•„íŒŒíŠ¸", "price": 7200, "target_price": 11000, "supply": 1261, "status": "ë¶„ì–‘ì¤‘"},
    {"id": "seoul_songpa_1", "name": "ì ì‹¤ ë˜ë¯¸ì•ˆ ì•„ì´íŒŒí¬", "address": "ì„œìš¸íŠ¹ë³„ì‹œ ì†¡íŒŒêµ¬ ì‹ ì²œë™", "brand": "ë˜ë¯¸ì•ˆ", "category": "ì•„íŒŒíŠ¸", "price": 5400, "target_price": 6200, "supply": 2678, "status": "ë¶„ì–‘ì¤‘"},
    {"id": "gyeonggi_uijeongbu_1", "name": "ì˜ì •ë¶€ íìŠ¤í…Œì´íŠ¸ íšŒë£¡ íŒŒí¬ë·°", "address": "ê²½ê¸°ë„ ì˜ì •ë¶€ì‹œ íšŒë£¡ë™", "brand": "íìŠ¤í…Œì´íŠ¸", "category": "ì•„íŒŒíŠ¸", "price": 1850, "target_price": 2100, "supply": 1816, "status": "ë¶„ì–‘ì¤‘"},
    {"id": "seoul_gangdong_3", "name": "ì´ì•ˆ ê°•ë™ ì»´í™ˆìŠ¤í…Œì´", "address": "ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë™êµ¬ ì²œí˜¸ë™", "brand": "ì´ì•ˆ", "category": "ì˜¤í”¼ìŠ¤í…”", "price": 2100, "target_price": 2350, "supply": 654, "status": "ì¤€ê³µì™„ë£Œ"},
    {"id": "daejeon_yuseong_1", "name": "ë„ì•ˆë¦¬ë²„íŒŒí¬ 1ë‹¨ì§€", "address": "ëŒ€ì „ê´‘ì—­ì‹œ ìœ ì„±êµ¬ í•™í•˜ë™", "brand": "íìŠ¤í…Œì´íŠ¸", "category": "ì•„íŒŒíŠ¸", "price": 1950, "target_price": 2250, "supply": 1124, "status": "ë¶„ì–‘ì¤‘"},
    {"id": "busan_gangseo_1", "name": "ë¶€ì‚° ì—ì½”ë¸íƒ€ì‹œí‹° 12BL", "address": "ë¶€ì‚°ê´‘ì—­ì‹œ ê°•ì„œêµ¬", "brand": "eí¸í•œì„¸ìƒ", "category": "ì•„íŒŒíŠ¸", "price": 1600, "target_price": 1950, "supply": 1258, "status": "ë¶„ì–‘ì¤‘"},
]

def create_db_and_tables():
    SQLModel.metadata.create_all(engine)
    
    # Migration: Add source column to lead table if it doesn't exist
    from sqlalchemy import text
    try:
        with engine.connect() as conn:
            # PRAGMA table_info returns (id, name, type, notnull, dflt_value, pk)
            columns = [row[1] for row in conn.execute(text("PRAGMA table_info(lead)")).fetchall()]
            if columns and 'source' not in columns:
                conn.execute(text("ALTER TABLE lead ADD COLUMN source TEXT DEFAULT 'ì•Œ ìˆ˜ ì—†ìŒ'"))
                conn.commit()
                logger.info("Database migration: Added 'source' column to 'lead' table.")
            
            # Site í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜
            site_columns = [row[1] for row in conn.execute(text("PRAGMA table_info(site)")).fetchall()]
            if site_columns:
                if 'down_payment' not in site_columns:
                    conn.execute(text("ALTER TABLE site ADD COLUMN down_payment TEXT DEFAULT '10%'"))
                if 'interest_benefit' not in site_columns:
                    conn.execute(text("ALTER TABLE site ADD COLUMN interest_benefit TEXT DEFAULT 'ì¤‘ë„ê¸ˆ ë¬´ì´ì'"))
                conn.commit()
                logger.info("Database migration: Added columns to 'site' table.")
    except Exception as e:
        logger.error(f"Migration error: {e}")

    with Session(engine) as session:
        for s_data in MOCK_SITES:
            existing = session.get(Site, s_data["id"])
            if not existing:
                session.add(Site(**s_data))
            else:
                for key, value in s_data.items():
                    setattr(existing, key, value)
        session.commit()

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì„œë²„ ê¸°ë™ ì‹œ DB ì´ˆê¸°í™” ë° CSV ë°ì´í„° ê¸°ë°˜ ê³ ì • ë°ì´í„° ë¡œë“œ
    create_db_and_tables()
    try:
        await import_csv_data()
        logger.info("Fixed site data loaded from sites_data.csv successfully.")
    except Exception as e:
        logger.error(f"Lifespan data load error: {e}")
    yield

app = FastAPI(lifespan=lifespan)

# CORS ì„¤ì •ì„ ë” ëª…ì‹œì ìœ¼ë¡œ ê°•í™”
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
    expose_headers=["*"],
)

class SiteSearchResponse(BaseModel):
    id: str
    name: str
    address: str
    status: Optional[str] = None
    brand: Optional[str] = None
    category: Optional[str] = None

@app.get("/search-sites", response_model=List[SiteSearchResponse])
async def search_sites(q: str):
    if not q or len(q) < 2:
        return []

    results = []
    seen_ids = set()
    q_lower = q.lower().strip()

    # 1. DB ê²€ìƒ‰ (ë¶„ì–‘ ë°ì´í„°ë² ì´ìŠ¤ ìš°ì„ )
    try:
        with Session(engine) as session:
            # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ê²€ìƒ‰ (name, address, brand, category, status ëª¨ë‘ ê²€ìƒ‰)
            statement = select(Site).where(
                or_(
                    col(Site.name).ilike(f"%{q}%"), 
                    col(Site.address).ilike(f"%{q}%"), 
                    col(Site.brand).ilike(f"%{q}%"),
                    col(Site.category).ilike(f"%{q}%"),
                    col(Site.status).ilike(f"%{q}%")
                )
            ).order_by(col(Site.last_updated).desc()).limit(100)
            db_sites = session.exec(statement).all()
            for s in db_sites:
                if s.id not in seen_ids:
                    results.append(SiteSearchResponse(id=s.id, name=s.name, address=s.address, status=s.status, brand=s.brand, category=s.category))
                    seen_ids.add(s.id)
    except Exception as e:
        logger.error(f"DB search error: {e}")

    # 2. ì‹¤ì‹œê°„ ë¶„ì–‘ ì „ë¬¸ API ê²€ìƒ‰ (êµ¬ì¶• ì•„íŒŒíŠ¸ë¥¼ ì›ì²œ ë°°ì œí•˜ê¸° ìœ„í•´ isale APIë§Œ ì‚¬ìš©)
    try:
        async with httpx.AsyncClient(follow_redirects=True) as client:
            fake_nnb = "".join(random.choices("0123456789ABCDEF", k=16))
            h = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "application/json, text/plain, */*",
                "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
                "Accept-Encoding": "gzip, deflate, br",
                "Referer": "https://m.land.naver.com/",
                "Origin": "https://m.land.naver.com",
                "Cookie": f"NNB={fake_nnb}",
                "Sec-Fetch-Dest": "empty",
                "Sec-Fetch-Mode": "cors",
                "Sec-Fetch-Site": "same-site"
            }
            
            # ë¶„ì–‘ ì •ë³´ê°€ ìˆëŠ” 'isale' ë°ì´í„°ë² ì´ìŠ¤ë§Œ ì¡°íšŒ (ì˜¤ë˜ëœ ê¸°ì¶• ì•„íŒŒíŠ¸ëŠ” ì—¬ê¸°ì„œ ê±¸ëŸ¬ì§)
            url_isale = "https://isale.land.naver.com/iSale/api/complex/searchList"
            params = {
                "keyword": q, 
                "complexType": "APT:ABYG:JGC:OR:OP:VL:DDD:ABC:ETC:UR:HO:SH", 
                "salesStatus": "0:1:2:3:4:5:6:7:8:9:10:11:12", 
                "pageSize": "100"
            }
            res_isale = await client.get(url_isale, params=params, headers=h, timeout=3.0)
            if res_isale.status_code == 200:
                data = res_isale.json()
                for it in data.get("result", {}).get("list", []):
                    sid = f"extern_isale_{it.get('complexNo')}"
                    if sid not in seen_ids:
                        results.append(SiteSearchResponse(
                            id=sid, name=it.get('complexName'), address=it.get('address'), 
                            status=it.get('salesStatusName'), brand=it.get('h_name'),
                            category=it.get('complexTypeName', 'ì•„íŒŒíŠ¸')
                        ))
                        seen_ids.add(sid)
    except Exception as e:
        logger.error(f"API search error: {e}")

    # ê²€ìƒ‰ ê²°ê³¼ ì •ë ¬ - í˜„ì¥ëª…ì— ê²€ìƒ‰ì–´ê°€ í¬í•¨ëœ ê²½ìš° ìš°ì„  í‘œì‹œ
    def sort_key(x):
        name_lower = x.name.lower() if x.name else ""
        address_lower = x.address.lower() if x.address else ""
        
        # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš° ìµœìš°ì„ 
        if name_lower == q_lower:
            return (0, 0)
        # í˜„ì¥ëª…ì´ ê²€ìƒ‰ì–´ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°
        if name_lower.startswith(q_lower):
            return (1, name_lower.find(q_lower))
        # í˜„ì¥ëª…ì— ê²€ìƒ‰ì–´ê°€ í¬í•¨ëœ ê²½ìš°
        if q_lower in name_lower:
            return (2, name_lower.find(q_lower))
        # ì£¼ì†Œì— ê²€ìƒ‰ì–´ê°€ í¬í•¨ëœ ê²½ìš°
        if q_lower in address_lower:
            return (3, address_lower.find(q_lower))
        # ê·¸ ì™¸
        return (999, 999)
    
    results.sort(key=sort_key)
    logger.info(f"Search query: '{q}' returned {len(results)} results")
    return results[:100]

@app.get("/force-csv-reload")
async def force_csv_reload():
    """ì—…ë¡œë“œëœ CSV íŒŒì¼ì„ ê¸°ì¤€ìœ¼ë¡œ DBë¥¼ ì™„ì „íˆ ê°•ì œ ê°±ì‹ í•©ë‹ˆë‹¤. (ì£¼ê°„ ì—…ë°ì´íŠ¸ ì‹œ í™œìš©)"""
    from sqlmodel import delete
    try:
        with Session(engine) as session:
            session.exec(delete(Site))
            session.commit()
        
        create_db_and_tables()
        result = await import_csv_data()
        return {"status": "success", "message": "CSV ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ DBê°€ ê°•ì œ ê°±ì‹ ë˜ì—ˆìŠµë‹ˆë‹¤.", "result": result}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/sync-external-naver")
async def sync_external_naver():
    """[ê´€ë¦¬ììš©] ë„¤ì´ë²„ ë¶€ë™ì‚° ë°ì´í„°ë¥¼ ìŠ¤ìº”í•˜ì—¬ DBì— ì„ì‹œ ì¶”ê°€í•©ë‹ˆë‹¤. (API ì°¨ë‹¨ ì£¼ì˜)"""
    # ... ê¸°ì¡´ sync_all ë¡œì§ ìœ ì§€ (í•„ìš” ì‹œì—ë§Œ ìˆ˜ë™ í˜¸ì¶œ)
    keywords = ["ë¶„ì–‘ê¶Œ", "ë¶„ì–‘", "ë¯¼ê°„ì„ëŒ€", "ì”ì—¬ì„¸ëŒ€", "ë¯¸ë¶„ì–‘"] 
    count = 0
    # (ì‹¤ì‹œê°„ì„±ë³´ë‹¤ëŠ” CSV ì—…ë¡œë“œë¥¼ ê¶Œì¥í•œë‹¤ëŠ” ë©”ì‹œì§€ í¬í•¨ ê°€ëŠ¥)
    return {"status": "deprecated", "message": "ì‹¤ì‹œê°„ ë™ê¸°í™” ëŒ€ì‹  ë¡œì»¬ì—ì„œ ìŠ¤ìº” í›„ CSV ì—…ë¡œë“œ ë°©ì‹ì„ ê¶Œì¥í•©ë‹ˆë‹¤."}

@app.get("/site-details/{site_id}")
async def get_site_details(site_id: str):
    with Session(engine) as session:
        site = session.get(Site, site_id)
        if site: return site
        return {"id": site_id, "name": "ë¶„ì–‘ ë¶„ì„ ì™„ë£Œ", "address": "ì§€ì—­ ì •ë³´", "brand": "ê¸°íƒ€", "category": "ë¶€ë™ì‚°", "price": 2500, "target_price": 2800, "supply": 500, "status": "ë°ì´í„° ë¡œë“œ"}

class AnalyzeRequest(BaseModel):
    field_name: Optional[str] = "ì•Œ ìˆ˜ ì—†ëŠ” í˜„ì¥"
    address: Optional[str] = "ì§€ì—­ ì •ë³´ ì—†ìŒ"
    product_category: Optional[str] = "ì•„íŒŒíŠ¸"
    sales_stage: Optional[str] = "ë¶„ì–‘ì¤‘"
    down_payment: Optional[Union[int, str]] = "10%"
    interest_benefit: Optional[str] = "ì—†ìŒ"
    additional_benefits: Optional[Union[List[str], str]] = []
    main_concern: Optional[str] = "ê¸°íƒ€"
    monthly_budget: Optional[Union[int, float, str]] = 0
    existing_media: Optional[Union[List[str], str]] = []
    sales_price: Optional[Union[float, str, int]] = 0.0
    target_area_price: Optional[Union[float, str, int]] = 0.0
    down_payment_amount: Optional[Union[int, float, str]] = 0
    supply_volume: Optional[Union[int, str]] = 0
    field_keypoints: Optional[str] = ""
    user_email: Optional[str] = None

class RegenerateCopyResponse(BaseModel):
    lms_copy_samples: List[str]
    channel_talk_samples: List[str]

@app.post("/regenerate-copy", response_model=RegenerateCopyResponse)
async def regenerate_copy(req: AnalyzeRequest):
    """Gemini AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¹´í”¼ë§Œ ì •ë°€í•˜ê²Œ ë‹¤ì‹œ ìƒì„±í•©ë‹ˆë‹¤."""
    field_name = req.field_name or "ë¶„ì„ í˜„ì¥"
    address = req.address or "ì§€ì—­ ì •ë³´"
    dp = str(req.down_payment) if req.down_payment else "10%"
    ib = req.interest_benefit or "ë¬´ì´ì"
    fkp = req.field_keypoints or "íƒì›”í•œ ì…ì§€ì™€ ë¯¸ë˜ê°€ì¹˜"
    
    prompt = f"""
    ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ìƒìœ„ 0.1% ë¶€ë™ì‚° í¼í¬ë¨¼ìŠ¤ ë§ˆì¼€íŒ… ë””ë ‰í„°ì´ì 'ë¶„ì–‘ ì•ŒíŒŒê³ 'ì˜ ìˆ˜ì„ ì „ëµê°€ì…ë‹ˆë‹¤. 
    [{field_name}] í˜„ì¥ì˜ ìˆ˜ë¶„ì–‘ ì˜í–¥ì„ ê·¹ëŒ€í™”í•˜ê³  DB ì „í™˜ìœ¨ì„ í­ë°œì ìœ¼ë¡œ ë†’ì´ê¸° ìœ„í•œ LMS(ë¬¸ì„œ) ë° ì±„ë„í†¡ ì¹´í”¼ 5ì¢…ì„ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

    [í˜„ì¥ í•µì‹¬ ë°ì´í„°]
    - í˜„ì¥ëª…: {field_name} / ìœ„ì¹˜: {address}
    - í•µì‹¬ íŠ¹ì¥ì : {fkp}
    - ê¸ˆìœµ í˜œíƒ: ê³„ì•½ê¸ˆ {dp}, {ib}
    
    [ì‘ì„± ìš”êµ¬ì‚¬í•­]
    1. LMS (5ì¢… ì„¸íŠ¸): 
       - 1ì•ˆ(ì‹ ë¢°/ë¸Œëœë“œ): ì¥ë¬¸ì˜ ì „ë¬¸ì„± ìˆëŠ” í†¤ì•¤ë§¤ë„ˆ, ê³µì‹ì  ë¶„ìœ„ê¸°.
       - 2ì•ˆ(ê¸ˆìœµ/ìˆ˜ìµ): ì£¼ë³€ ì‹œì„¸ ëŒ€ë¹„ ì €ë ´í•œ ë¶„ì–‘ê°€, ì´ì í˜œíƒ ë“± ìˆ˜ìµì„± ê°•ì¡°.
       - 3ì•ˆ(ê¸´ê¸‰/í›„í‚¹): ë§ˆê° ì„ë°•, ë¡œì—´ì¸µ ì†Œì§„ ë“± ì‹¬ë¦¬ì  íŠ¸ë¦¬ê±° í™œìš©.
       - 4ì•ˆ(ì…ì§€/ë¹„ì „): ë¯¸ë˜ ê°€ì¹˜, ê°œë°œ í˜¸ì¬, êµí†µë§ ë¶€ê°.
       - 5ì•ˆ(ê°ì„±/ë¼ì´í”„ìŠ¤íƒ€ì¼): ê±°ì£¼ ë§Œì¡±ë„, íŠ¹í™” ì„¤ê³„, ì‚¶ì˜ ì§ˆ ê°•ì¡°.
    2. ì±„ë„í†¡ (5ì¢… ì„¸íŠ¸):
       - ëª¨ë°”ì¼ ì•±(í˜¸ê°±ë…¸ë…¸, ì§ë°© ë“±) ìœ ì €ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ í•œ ì§§ê³  í•µì‹¬ì ì¸ ë¬¸êµ¬.
       - ì´ëª¨ì§€ë¥¼ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ í´ë¦­ìœ¨(CTR)ì„ ê·¹ëŒ€í™”í•˜ì‹­ì‹œì˜¤.

    [ì¶œë ¥ í¬ë§·: JSON]
    {{
        "lms_copy_samples": ["LMS 1ì•ˆ", "LMS 2ì•ˆ", "LMS 3ì•ˆ", "LMS 4ì•ˆ", "LMS 5ì•ˆ"],
        "channel_talk_samples": ["ì±„ë„í†¡ 1ì•ˆ", "ì±„ë„í†¡ 2ì•ˆ", "ì±„ë„í†¡ 3ì•ˆ", "ì±„ë„í†¡ 4ì•ˆ", "ì±„ë„í†¡ 5ì•ˆ"]
    }}
    """
    
    ai_data = None
    model_candidates = ['gemini-flash-latest', 'gemini-pro-latest', 'gemini-2.0-flash-lite']
    
    for model_name in model_candidates:
        try:
            logger.info(f"Regenerate copy attempt with: {model_name}")
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt)
            if response and response.text:
                ai_data = extract_json(response.text)
                if ai_data: break
        except Exception as e:
            logger.error(f"Regenerate copy model {model_name} failed: {e}")
            continue

    if ai_data:
        lms_res = ai_data.get("lms_copy_samples", [])
        if not isinstance(lms_res, list): lms_res = []
        lms_res = [str(x) for x in lms_res if x]
        while len(lms_res) < 5: lms_res.append(f"{field_name} ì¶”ê°€ ì¹´í”¼ ì¤€ë¹„ ì¤‘...")
        
        chn_res = ai_data.get("channel_talk_samples", [])
        if not isinstance(chn_res, list): chn_res = []
        chn_res = [str(x) for x in chn_res if x]
        while len(chn_res) < 5: chn_res.append(f"{field_name} ì¶”ê°€ ì±„ë„í†¡ ì¤€ë¹„ ì¤‘...")

        return RegenerateCopyResponse(
            lms_copy_samples=lms_res[:5],
            channel_talk_samples=chn_res[:5]
        )
    
    # Fallback to smart templates
    gap_percent = 15
    lms_samples = [
        f"ã€{field_name}ã€‘\n\nğŸ”¥ íŒŒê²©ì¡°ê±´ë³€ê²½!!\nâ˜› ê³„ì•½ê¸ˆ {dp}\nâ˜› {ib} íŒŒê²© í˜œíƒ\nâ˜› ì‹¤ê±°ì£¼ì˜ë¬´ ë° ì²­ì•½í†µì¥ ç„¡\n\nâ–  ë¸Œëœë“œ & ìì‚° ê°€ì¹˜\nâ–¶ ì£¼ë³€ ì‹œì„¸ ëŒ€ë¹„ {gap_percent}% ë‚®ì€ ì••ë„ì  ë¶„ì–‘ê°€\nâ–¶ {fkp} íŠ¹í™” ì„¤ê³„ ì ìš©\nâ˜ï¸ ë¬¸ì˜ : 1600-0000",
        f"[íŠ¹ë³„ê³µì‹ë°œì†¡] {field_name} ê´€ì‹¬ê³ ê° ì•ˆë‚´\nğŸ’° ê°•ë ¥í•œ ê¸ˆìœµ í˜œíƒ\nâœ… ê³„ì•½ê¸ˆ {dp}\nâœ… {ib}\nâ˜ï¸ ìƒë‹´ë¬¸ì˜: 010-0000-0000",
        f"ğŸš¨ {field_name} ì œë¡œê³„ì•½ê¸ˆ ìˆ˜ì¤€ ë§ˆê° ì„ë°•!\nğŸ”¥ {ib}, ì£¼íƒìˆ˜ ë¯¸í¬í•¨ ìˆ˜í˜œ\nğŸ“ ëŒ€í‘œë²ˆí˜¸: 1811-0000",
        f"ğŸ’ {field_name} ë¯¸ë˜ê°€ì¹˜ ë¦¬í¬íŠ¸ ê³µê°œ\nğŸ™ï¸ {address}ì˜ í•µì‹¬ ìˆ˜í˜œì§€\nğŸ“‰ í•©ë¦¬ì  {gap_percent}% ë‚®ì€ ê°€ê²©\nâ–¶ ìƒì„¸ ë‚´ìš©: [ìƒë‹´ë¬¸ì˜]",
        f"ğŸ¢ {field_name} í”„ë¦¬ë¯¸ì—„ í‰ë©´ ì•ˆë‚´\nâœ¨ ì „ì„¸ëŒ€ í¬ë² ì´ íŠ¹í™” ì„¤ê³„\nğŸŒ³ ì£¼ê±° ë§Œì¡±ë„ 1ìœ„ì˜ ê°€ì¹˜\nâ˜ï¸ ëŒ€í‘œë¬¸ì˜: 0507-0000-0000"
    ]
    channel_samples = [
        f"ğŸ”¥ {field_name} | íŒŒê²© ì¡°ê±´ë³€ê²½ ì†Œì‹!\nâœ… í•µì‹¬ í˜œíƒ ìš”ì•½:\n- ê³„ì•½ê¸ˆ {dp}\n- ì´ì ë¶€ë‹´ ì œë¡œ! {ib} í™•ì •\nğŸ“¢ ì‹¤ì‹œê°„ ë¡œì—´ì¸µ í™•ì¸ ğŸ‘‡",
        f"ğŸš¨ [ê¸´ê¸‰] {field_name} ë¡œì—´ì¸µ ì„ ì°©ìˆœ ë§ˆê° ì§ì „!\nğŸ“ ê¸´ê¸‰ ìƒë‹´ ë° ë°©ë¬¸ì˜ˆì•½: 010-0000-0000",
        f"ğŸ“Š {field_name} ê³ ê´€ì—¬ ì‹¤ê±°ì£¼ìš© [ì •ë°€ ë¶„ì„ ë¦¬í¬íŠ¸]\n{fkp} ë“± ì£¼ê±° ë§Œì¡±ë„ 1ìœ„ì˜ ì§„ì§œ ì´ìœ ë¥¼ ë¦¬í¬íŠ¸ë¡œ í™•ì¸í•˜ì„¸ìš”. ğŸ’",
        f"ğŸ—ï¸ {address}ì˜ ë¯¸ë˜ [{field_name}]\nğŸ’ ëœë“œë§ˆí¬ ì…ì§€ í”„ë¦¬ë¯¸ì—„ ê³µê°œ",
        f"ğŸ [{field_name}] ì´ë²¤íŠ¸ ì°¸ì—¬\nëª¨ë¸í•˜ìš°ìŠ¤ ë°©ë¬¸ ì‹œ íŠ¹ë³„ ì„ ë¬¼ ì¦ì • âœ¨"
    ]
    return RegenerateCopyResponse(lms_copy_samples=lms_samples, channel_talk_samples=channel_samples)

@app.post("/analyze")
async def analyze_site(request: Optional[AnalyzeRequest] = None):
    """Gemini AIë¥¼ ì‚¬ìš©í•œ í˜„ì¥ ì •ë°€ ë¶„ì„ API (ê³ ë„í™” ë²„ì „)"""
    # ê¸°ë³¸ê°’ ì„¤ì • (fallback ì‹œ NameError ë°©ì§€)
    field_name = "ë¶„ì„ í˜„ì¥"
    address = "ì§€ì—­ ì •ë³´ ì—†ìŒ"
    product_category = "ì•„íŒŒíŠ¸"
    sales_price = 0.0
    target_price = 0.0
    market_gap = 0.0
    gap_percent = 0.0
    gap_status = "ë†’ì€"
    supply_volume = 0
    field_keypoints = ""
    ib = "ë¬´ì´ì"
    dp = "10%"
    main_concern = "ê¸°íƒ€"

    logger.info(f">>> Analyze request received: {request.field_name if request else 'No request body'}")
    
    try:
        req = request if request else AnalyzeRequest()
        
        field_name = getattr(req, 'field_name', "ë¶„ì„ í˜„ì¥")
        address = getattr(req, 'address', "ì§€ì—­ ì •ë³´ ì—†ìŒ")
        product_category = getattr(req, 'product_category', "ì•„íŒŒíŠ¸")
        
        # ìˆ«ì í•„ë“œ ì•ˆì „í•˜ê²Œ ë³€í™˜
        try:
            sales_price = float(req.sales_price or 0.0)
        except: sales_price = 0.0
        
        try:
            target_price = float(req.target_area_price or 0.0)
        except: target_price = 0.0
        
        market_gap = target_price - sales_price
        gap_status = "ì €ë ´" if market_gap > 0 else "ë†’ì€"
        gap_percent = abs(round((market_gap / (sales_price if sales_price > 0 else 1)) * 100, 1))
        
        # supply_volume ì²˜ë¦¬ (ë¬¸ìì—´ í¬í•¨ ì‹œ ìˆ«ìë§Œ ì¶”ì¶œ)
        try:
            sv_raw = str(req.supply_volume or "0")
            sv_digits = "".join(filter(str.isdigit, sv_raw))
            supply_volume = int(sv_digits) if sv_digits else 0
        except:
            supply_volume = 0
            
        main_concern = req.main_concern or "ê¸°íƒ€"
        field_keypoints = getattr(req, 'field_keypoints', "")
        dp = str(req.down_payment) if req.down_payment else "10%"
        ib = req.interest_benefit or "ë¬´ì´ì"
        fkp = field_keypoints if field_keypoints else "íƒì›”í•œ ì…ì§€ì™€ ë¯¸ë˜ê°€ì¹˜"
        
        # 1. ì‹¤ì‹œê°„ ì—¬ë¡  ë° ë°ì´í„° ìˆ˜ì§‘
        search_context = ""
        try:
            async with httpx.AsyncClient() as client:
                search_url = "https://search.naver.com/search.naver"
                search_params = {"query": f"{field_name} ë¶„ì–‘ê°€ ëª¨ë¸í•˜ìš°ìŠ¤", "where": "view"}
                h = {"User-Agent": "Mozilla/5.0"}
                res = await client.get(search_url, params=search_params, headers=h, timeout=4.0)
                if res.status_code == 200:
                    search_context = res.text[:3000]
        except Exception as e:
            logger.warning(f"Live search skipped: {e}")

        # 2. AI ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì‘ì„±
        prompt = f"""
        ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë¶€ë™ì‚° ë¶„ì–‘ ë§ˆì¼€íŒ…ì˜ ì ˆëŒ€ê°•ì 'ë¶„ì–‘ ì•ŒíŒŒê³ ' ì‹œìŠ¤í…œì…ë‹ˆë‹¤. 
        [{field_name}] í˜„ì¥ì˜ ì„±ê³µì ì¸ ë¶„ì–‘ì„ ìœ„í•œ 'ì •ë°€ ì‹œì¥ ë¶„ì„' ë° 'íŠ¹í™” ë§ˆì¼€íŒ… ê°€ì´ë“œ'ë¥¼ ì „ë¬¸ê°€ ìˆ˜ì¤€ìœ¼ë¡œ ìƒì„¸í•˜ê²Œ JSONìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

        [ë°ì´í„° ì„¸íŠ¸]
        - í˜„ì¥ëª…: {field_name} / ìœ„ì¹˜: {address} / ìƒí’ˆêµ°: {product_category}
        - í”„ë¼ì´ì‹±: ê³µê¸‰ê°€ {sales_price} VS ì£¼ë³€ ì‹œì„¸ {target_price}
        - ê³µê¸‰ ê·œëª¨: {supply_volume}ì„¸ëŒ€
        - ê¸ˆìœµ ì¡°ê±´: ê³„ì•½ê¸ˆ {dp}, {ib}
        - í•µì‹¬ íŠ¹ì¥ì : {fkp}
        - í˜„ì¬ ë§ˆì¼€íŒ… ê³ ë¯¼: {main_concern}
        
        [ê²€ìƒ‰ì°¸ê³  ë°ì´í„°] 
        {search_context[:1000] if search_context else "ìµœì‹  ê²€ìƒ‰ íŠ¸ë Œë“œ ê¸°ë°˜ ë¶„ì„ í•„ìš”"}

        [ë¯¸ì…˜ ë° ì¶œë ¥ ìš”êµ¬ì‚¬í•­]
        1. market_diagnosis: í˜„ì¬ì˜ ê±°ì‹œ ê²½ì œ íë¦„ê³¼ í•´ë‹¹ ì§€ì—­ì˜ êµ¬ì²´ì  ì§€í‘œë¥¼ ê²°í•©í•œ ë‚ ì¹´ë¡œìš´ í†µì°°ë ¥ì„ ì œê³µí•˜ì‹­ì‹œì˜¤.
        2. media_mix: 'í˜¸ê°±ë…¸ë…¸ ì±„ë„í†¡', 'LMS(ë¬¸ì ë§ˆì¼€íŒ…)'ë¥¼ í¬í•¨í•œ ìµœì ì˜ 3ê°œ ë§¤ì²´ ì „ëµì„ ì œì‹œí•˜ì‹­ì‹œì˜¤.
        3. lms_copy_samples & channel_talk_samples: ìœ„ ë§¤ì²´ì— íŠ¹í™”ëœ ê³ íš¨ìœ¨ ì¹´í”¼ ê° 5ì¢…ì„ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

        [JSON Output Structure]
        {{
            "market_diagnosis": "...",
            "target_persona": "...",
            "target_audience": ["#íƒœê·¸1", "#íƒœê·¸2", "#íƒœê·¸3", "#íƒœê·¸4", "#íƒœê·¸5"],
            "competitors": [
                {{"name": "ì¸ê·¼ ë¹„êµ ë‹¨ì§€ A", "price": {target_price or 0}, "gap_label": "ë„ë³´ 5ë¶„"}},
                {{"name": "ì¸ê·¼ ë¹„êµ ë‹¨ì§€ B", "price": {target_price * 1.05 if target_price else 0}, "gap_label": "1.2km ì¸ì ‘"}}
            ],
            "ad_recommendation": "...",
            "copywriting": "...",
            "keyword_strategy": ["í‚¤ì›Œë“œ1", "2", "3", "4", "5"],
            "weekly_plan": ["1ì£¼ì°¨", "2ì£¼ì°¨", "3ì£¼ì°¨", "4ì£¼ì°¨"],
            "roi_forecast": {{"expected_leads": 150, "expected_cpl": 45000, "conversion_rate": 3.5, "expected_ctr": 1.9}},
            "lms_copy_samples": ["ì¹´í”¼1", "ì¹´í”¼2", "ì¹´í”¼3", "ì¹´í”¼4", "ì¹´í”¼5"],
            "channel_talk_samples": ["ì±„ë„1", "ì±„ë„2", "ì±„ë„3", "ì±„ë„4", "ì±„ë„5"],
            "media_mix": [
                {{"media": "ë§¤ì²´ëª…", "feature": "ê°•ì ", "reason": "ì´ìœ ", "strategy_example": "ì „ëµ"}}
            ]
        }}
        """

        ai_data = None
        model_candidates = [
            'gemini-flash-latest',
            'gemini-pro-latest',
            'gemini-2.0-flash-lite'
        ]
        
        for model_name in model_candidates:
            try:
                logger.info(f"Attempting AI analysis with model: {model_name}")
                model = genai.GenerativeModel(model_name)
                response = model.generate_content(prompt)
                if response and response.text:
                    ai_data = extract_json(response.text)
                    if ai_data: 
                        logger.info(f"Success with model: {model_name}")
                        break
            except Exception as e:
                logger.error(f"Model {model_name} failed: {e}")
                continue

        if not ai_data:
            logger.warning("AI model failed. Triggering Smart Local Engine.")
            raise Exception("AI Response Parsing Failed")

        # í•„ìˆ˜ í•„ë“œ ëˆ„ë½ ë°©ì§€ ë° ê¸°ë³¸ê°’ ë³´ì •
        safe_data = {
            "market_diagnosis": ai_data.get("market_diagnosis") or "ë°ì´í„° ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤.",
            "target_audience": ai_data.get("target_audience") or ["ì‹¤ê±°ì£¼ì", "íˆ¬ìì"],
            "target_persona": ai_data.get("target_persona") or "ì•ˆì •ì  ìì‚° ì¦ì‹ì„ ë…¸ë¦¬ëŠ” ìˆ˜ìš”ì",
            "competitors": ai_data.get("competitors") or [],
            "ad_recommendation": ai_data.get("ad_recommendation") or "ë©”íƒ€ ë° ë„¤ì´ë²„ ê´‘ê³  ì§‘í–‰ ê¶Œì¥",
            "copywriting": ai_data.get("copywriting") or f"[{field_name}] ì§€ê¸ˆ ë°”ë¡œ ë§Œë‚˜ë³´ì„¸ìš”.",
            "keyword_strategy": ai_data.get("keyword_strategy") or [field_name, "ë¶„ì–‘ì •ë³´"],
            "weekly_plan": ai_data.get("weekly_plan") or ["1ì£¼ì°¨: ë§ˆì¼€íŒ… ê¸°íš"],
            "roi_forecast": ai_data.get("roi_forecast") or {"expected_leads": 100, "expected_cpl": 50000, "conversion_rate": 2.5, "expected_ctr": 1.8},
            "lms_copy_samples": ai_data.get("lms_copy_samples") or [],
            "channel_talk_samples": ai_data.get("channel_talk_samples") or [],
            "media_mix": ai_data.get("media_mix") or []
        }
        
        # media_mix ë‚´ë¶€ í•„ë“œ ë³´ì •
        final_media_mix = []
        for m in safe_data["media_mix"]:
            if isinstance(m, dict):
                final_media_mix.append({
                    "media": str(m.get("media", "ë§¤ì²´")),
                    "feature": str(m.get("feature", "íŠ¹ì§•")),
                    "reason": str(m.get("reason", "ë¶„ì„ ì‚¬ìœ ")),
                    "strategy_example": str(m.get("strategy_example", "ì „ëµ ì˜ˆì‹œ"))
                })
        safe_data["media_mix"] = final_media_mix
        if "expected_ctr" not in safe_data["roi_forecast"]:
            safe_data["roi_forecast"]["expected_ctr"] = 1.8

        # ROI Forecast í•„ë“œ ë³´ì •
        default_roi = {"expected_leads": 100, "expected_cpl": 50000, "conversion_rate": 2.5, "expected_ctr": 1.8}
        if not isinstance(safe_data.get("roi_forecast"), dict):
            safe_data["roi_forecast"] = default_roi
        else:
            for k, v in default_roi.items():
                if k not in safe_data["roi_forecast"]:
                    safe_data["roi_forecast"][k] = v
                else:
                    try:
                        safe_data["roi_forecast"][k] = float(safe_data["roi_forecast"][k])
                    except:
                        safe_data["roi_forecast"][k] = v

        # ë¦¬ìŠ¤íŠ¸ í•„ë“œ ë³´ì •
        for key in ["lms_copy_samples", "channel_talk_samples", "target_audience", "weekly_plan", "keyword_strategy"]:
            val = safe_data.get(key)
            if isinstance(val, str):
                val = [val]
            elif not isinstance(val, list):
                val = []
            
            # ì¹´í”¼ ìƒ˜í”Œì€ ë°˜ë“œì‹œ 5ê°œ ë³´ì¥
            if key in ["lms_copy_samples", "channel_talk_samples"]:
                val = [str(x) for x in val if x]
                while len(val) < 5:
                    val.append(f"{field_name} íŠ¹í™” ë¶„ì„ ì¹´í”¼ ìƒì„± ëŒ€ê¸° ì¤‘...")
                val = val[:5]
            else:
                val = [str(x) for x in val]
                
            safe_data[key] = val

        # competitors í•„ë“œ ë³´ì •
        final_competitors = []
        for c in safe_data["competitors"]:
            if isinstance(c, dict):
                try:
                    p_val = float(c.get("price", 0))
                except: p_val = 0.0
                
                final_competitors.append({
                    "name": str(c.get("name", "ê²½ìŸ ë‹¨ì§€")),
                    "price": p_val,
                    "gap_label": str(c.get("gap_label") or c.get("distance") or "ë¹„êµêµ°")
                })
        safe_data["competitors"] = final_competitors

        price_score = min(100, max(0, 100 - abs(sales_price - target_price) / (target_price if target_price > 0 else 1) * 100))
        location_score = 75 + random.randint(-5, 10)
        benefit_score = 70 + random.randint(-5, 10)
        total_score = int((price_score * 0.4 + location_score * 0.3 + benefit_score * 0.3))

        return {
            "score": int(total_score),
            "score_breakdown": {
                "price_score": int(price_score),
                "location_score": int(location_score),
                "benefit_score": int(benefit_score),
                "total_score": int(total_score)
            },
            "market_diagnosis": safe_data["market_diagnosis"],
            "market_gap_percent": round(gap_percent, 2),
            "price_data": [
                {"name": "ìš°ë¦¬ í˜„ì¥", "price": sales_price},
                {"name": "ì£¼ë³€ ì‹œì„¸", "price": target_price},
                {"name": "ì‹œì„¸ ì°¨ìµ", "price": abs(target_price - sales_price)}
            ],
            "radar_data": [
                {"subject": "ë¶„ì–‘ê°€", "A": int(price_score), "B": 70, "fullMark": 100},
                {"subject": "ë¸Œëœë“œ", "A": 85, "B": 75, "fullMark": 100},
                {"subject": "ë‹¨ì§€ê·œëª¨", "A": min(100, (supply_volume // 10) + 20), "B": 60, "fullMark": 100},
                {"subject": "ì…ì§€", "A": int(location_score), "B": 65, "fullMark": 100},
                {"subject": "ë¶„ì–‘ì¡°ê±´", "A": 80, "B": 50, "fullMark": 100},
                {"subject": "ìƒí’ˆì„±", "A": int(benefit_score), "B": 70, "fullMark": 100}
            ],
            "target_persona": safe_data["target_persona"],
            "target_audience": safe_data["target_audience"],
            "competitors": safe_data["competitors"],
            "ad_recommendation": safe_data["ad_recommendation"],
            "copywriting": safe_data["copywriting"],
            "keyword_strategy": safe_data["keyword_strategy"],
            "weekly_plan": safe_data["weekly_plan"],
            "roi_forecast": safe_data["roi_forecast"],
            "lms_copy_samples": safe_data["lms_copy_samples"],
            "channel_talk_samples": safe_data["channel_talk_samples"],
            "media_mix": safe_data["media_mix"] if safe_data["media_mix"] else [
                {"media": "ë©”íƒ€/ì¸ìŠ¤íƒ€", "feature": "ì •ë°€ íƒ€ì¼“íŒ…", "reason": "ê´€ì‹¬ì‚¬ ê¸°ë°˜ ë„ë‹¬", "strategy_example": "í˜œíƒ ê°•ì¡° ê´‘ê³ "},
                {"media": "ë„¤ì´ë²„", "feature": "ê²€ìƒ‰ ê¸°ë°˜", "reason": "êµ¬ë§¤ ì˜í–¥ ê³ ê° í™•ë³´", "strategy_example": "ì§€ì—­ í‚¤ì›Œë“œ ì ìœ "},
                {"media": "ì¹´ì¹´ì˜¤", "feature": "ëª¨ë¨¼íŠ¸ íƒ€ê²Ÿ", "reason": "ì§€ì—­ ê¸°ë°˜ ë…¸ì¶œ", "strategy_example": "ë°©ë¬¸ ìœ ë„"}
            ]
        }
    except Exception as e:
        import traceback
        logger.error(f"Critical analyze error: {e}\n{traceback.format_exc()}")
        
        cat_msg = "ì£¼ê±° ì„ í˜¸ë„ê°€ ë†’ì€ ì•„íŒŒíŠ¸" if "ì•„íŒŒíŠ¸" in product_category else "ìˆ˜ìµí˜• ë¶€ë™ì‚°ìœ¼ë¡œì„œ ê°€ì¹˜ê°€ ë†’ì€ ìƒí’ˆ"
        smart_diagnosis = (
            f"[{field_name}]ì€ ì¸ê·¼ ì‹œì„¸({target_price}ë§Œì›) ëŒ€ë¹„ ì•½ {gap_percent}% {gap_status}í•œ ê°€ê²©ëŒ€ë¡œ ì±…ì •ë˜ì–´ ì‹¤ê±°ì£¼ ë° íˆ¬ì ìˆ˜ìš”ì˜ ìœ ì…ì´ ë§¤ìš° ê°•ë ¥í•  ê²ƒìœ¼ë¡œ ì˜ˆì¸¡ë©ë‹ˆë‹¤. "
            f"íŠ¹íˆ {address} ë‚´ì—ì„œë„ {cat_msg}ë¡œ ë¶„ë¥˜ë˜ì–´ ì…ì§€ì  í¬ì†Œì„±ì´ ë‹ë³´ì´ë©°, {field_keypoints if field_keypoints else 'íƒì›”í•œ ì…ì§€'}ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ˆê¸° ë¶„ì–‘ë¥  80% ì´ìƒì„ ëª©í‘œë¡œ í•˜ëŠ” ê³µê²©ì ì¸ ë§ˆì¼€íŒ…ì´ ìœ íš¨í•œ ì‹œì ì…ë‹ˆë‹¤. "
            f"ì£¼ë³€ {product_category} ê³µê¸‰ëŸ‰ê³¼ ëŒ€ë¹„í•´ ë³´ì•˜ì„ ë•Œ ì‹œì„¸ ì°¨ìµ ì•½ {abs(market_gap):.0f}ë§Œì›ì˜ í”„ë¦¬ë¯¸ì—„ í™•ë³´ê°€ ê°€ëŠ¥í•˜ë¯€ë¡œ, ì´ë¥¼ í•µì‹¬ ì†Œêµ¬ì ìœ¼ë¡œ í•œ í¼í¬ë¨¼ìŠ¤ ê´‘ê³  ì§‘í–‰ì„ ì ê·¹ ê¶Œì¥í•©ë‹ˆë‹¤."
        )

        return {
            "score": 85,
            "score_breakdown": {
                "price_score": 90 if market_gap > 0 else 70,
                "location_score": 82,
                "benefit_score": 88,
                "total_score": 85
            },
            "market_diagnosis": smart_diagnosis,
            "market_gap_percent": round(gap_percent, 2),
            "price_data": [
                {"name": "ìš°ë¦¬ í˜„ì¥", "price": sales_price},
                {"name": "ì£¼ë³€ ì‹œì„¸", "price": target_price},
                {"name": "ì‹œì„¸ ì°¨ìµ", "price": abs(target_price - sales_price)}
            ],
            "radar_data": [
                {"subject": "ë¶„ì–‘ê°€", "A": 90 if market_gap > 0 else 72, "B": 70, "fullMark": 100},
                {"subject": "ë¸Œëœë“œ", "A": 85, "B": 75, "fullMark": 100},
                {"subject": "ë‹¨ì§€ê·œëª¨", "A": min(100, (supply_volume // 10) + 30), "B": 60, "fullMark": 100},
                {"subject": "ì…ì§€", "A": 80, "B": 65, "fullMark": 100},
                {"subject": "ë¶„ì–‘ì¡°ê±´", "A": 80, "B": 50, "fullMark": 100},
                {"subject": "ìƒí’ˆì„±", "A": 90, "B": 70, "fullMark": 100}
            ],
            "target_persona": f"{address} ì¸ê·¼ ì‹¤ê±°ì£¼ë¥¼ í¬ë§í•˜ëŠ” 3040 ë§ë²Œì´ ë¶€ë¶€ ë° ì•ˆì •ì  ìì‚° ì¦ì‹ì„ ë…¸ë¦¬ëŠ” 50ëŒ€ íˆ¬ìì",
            "target_audience": ["#ë‚´ì§‘ë§ˆë ¨", "#ì‹¤ìˆ˜ìš”ì", f"#{address.split()[0] if address and address.split() else 'ë¶„ì–‘'}", "#í”„ë¦¬ë¯¸ì—„", "#ë¶„ì–‘ì •ë³´"],
            "competitors": [
                {"name": "ì¸ê·¼ ë¹„êµ ë‹¨ì§€ A", "price": target_price, "gap_label": "1.1km ì¸ì ‘"},
                {"name": "ì¸ê·¼ ë¹„êµ ë‹¨ì§€ B", "price": round(target_price * 1.05), "gap_label": "ë„ë³´ 15ë¶„"}
            ],
            "ad_recommendation": "ë„¤ì´ë²„ ë¸Œëœë“œê²€ìƒ‰ì„ í†µí•œ ì‹ ë¢°ë„ í™•ë³´ì™€ ë©”íƒ€/ì¸ìŠ¤íƒ€ì˜ 'ì‹œì„¸ì°¨ìµ' ê°•ì¡° ë¦¬ë“œê´‘ê³  ë¹„ì¤‘ 7:3 ì§‘í–‰ ê¶Œì¥",
            "copywriting": f"[{field_name}] ì£¼ë³€ ì‹œì„¸ë³´ë‹¤ {gap_percent}% ë” ê°€ë³ê²Œ! ë§ˆí¬ì˜ ìƒˆë¡œìš´ ì¤‘ì‹¬ì„ ì„ ì í•˜ì‹­ì‹œì˜¤.",
            "keyword_strategy": [field_name, f"{field_name} ë¶„ì–‘ê°€", f"{address.split()[0]} ì‹ ì¶•ì•„íŒŒíŠ¸", "ì²­ì•½ì¼ì •", "ëª¨ë¸í•˜ìš°ìŠ¤ìœ„ì¹˜"],
            "weekly_plan": [
                "1ì£¼: í‹°ì§• ê´‘ê³  ë° ê´€ì‹¬ê³ ê° DB 300ê±´ í™•ë³´ ëª©í‘œ",
                "2ì£¼: ë¶„ì–‘ê°€ ë° í˜œíƒ ê°•ì¡° ì •ë°€ íƒ€ê²ŸíŒ… ìº í˜ì¸ í™•ì‚°",
                "3ì£¼: ëª¨ë¸í•˜ìš°ìŠ¤ ë°©ë¬¸ ì˜ˆì•½ ì´ë²¤íŠ¸ ë° ì§‘ì¤‘ DB ê´€ë¦¬",
                "4ì£¼: ì²­ì•½ ì „ ë§ˆê° ì…ë°• ë©”ì‹œì§€ ë° ìµœì¢… ìƒë‹´ ì „í™˜ í™œë™"
            ],
            "roi_forecast": {"expected_leads": 120, "expected_cpl": 48000, "expected_ctr": 1.7, "conversion_rate": 3.2},
            "lms_copy_samples": [
                f"ã€{field_name}ã€‘\n\nğŸ”¥ íŒŒê²©ì¡°ê±´ë³€ê²½!!\nâ˜› ê³„ì•½ê¸ˆ {dp}\nâ˜› {ib} í˜œíƒ í™•ì •\nâ˜› ì‹¤ê±°ì£¼ì˜ë¬´ ë° ì „ë§¤ì œí•œ í•´ì œ\n\nâ–  ë¸Œëœë“œ & ìì‚° ê°€ì¹˜\nâ–¶ ì£¼ë³€ ì‹œì„¸ ëŒ€ë¹„ {gap_percent}% ë‚®ì€ ì••ë„ì  ë¶„ì–‘ê°€\nâ–¶ {fkp if fkp else 'í”„ë¦¬ë¯¸ì—„ íŠ¹í™” ì„¤ê³„'} ì ìš©\nâ–¶ {supply_volume}ì„¸ëŒ€ ëœë“œë§ˆí¬ ìŠ¤ì¼€ì¼\n\nâ˜ï¸ ê³µì‹ë¬¸ì˜ : 1600-0000",
                f"[ê³µì‹ë³¸ë¶€ë°œì†¡] {field_name} ë¡œì—´ì¸µ ì„ ì°©ìˆœ ì•ˆë‚´\nğŸ’° ê°•ë ¥í•œ ê¸ˆìœµ í˜œíƒ\nâœ… ê³„ì•½ê¸ˆ ì •ì•¡ì œ ì‹¤ì‹œ\nâœ… {ib}\nâœ… ë¬´ì œí•œ ì „ë§¤ ê°€ëŠ¥\n\nğŸ¡ í˜„ì¥ íŠ¹ì¥ì \n- {address} ë‚´ ë§ˆì§€ë§‰ ë…¸ë‹¤ì§€ í•µì‹¬ í™©ê¸ˆ ìë¦¬\n- ì‹œì„¸ ì°¨ìµ ì•½ {abs(market_gap):.0f}ë§Œì›ì˜ ê°•ë ¥í•œ ê°€ì¹˜\nâ˜ï¸ ëŒ€í‘œë²ˆí˜¸: 010-0000-0000",
                f"ğŸš¨ {field_name} ë§ˆê° ì„ë°• ì•ˆë‚´!\nğŸ”¥ ì „ì„¸ëŒ€ ì˜êµ¬ íŒŒë…¸ë¼ë§ˆ ì¡°ë§\nğŸ”¥ ì¸ê¸° íƒ€ì… ì™„íŒ ì§ì „\nğŸ”¥ {ib} ìˆ˜í˜œ\n\nğŸ“ ê¸´ê¸‰ë¬¸ì˜: 1800-0000",
                f"ğŸ’ [{field_name}] ë¯¸ë˜ê°€ì¹˜ ë¦¬í¬íŠ¸ ë°œì†¡\nğŸ™ï¸ {address}ì˜ ì¤‘ì‹¬, ë‹¤ì‹œ ì—†ì„ ê¸°íšŒ\nğŸ“‰ {gap_status} ê°€ê²©ëŒ€ë¡œ ì„ ì í•˜ëŠ” ë‚´ì§‘ë§ˆë ¨\nğŸš€ GTX/êµí†µ í˜¸ì¬ì˜ ì§ì ‘ ìˆ˜í˜œì§€\nâ–¶ ë¦¬í¬íŠ¸ í™•ì¸: [ìƒë‹´ì˜ˆì•½]",
                f"ğŸ  [{field_name}] ë¼ì´í”„ìŠ¤íƒ€ì¼ì˜ ì™„ì„±\nâœ¨ {fkp if fkp else 'ìµœê³ ê¸‰ ì»¤ë®¤ë‹ˆí‹°'}ë¥¼ ê°–ì¶˜ ëŒ€ë‹¨ì§€\nğŸŒ¿ ë„ì‹¬ ì† íë§ ë¼ì´í”„, ìˆ²ì„¸ê¶Œ ê°€ì¹˜\nğŸ’ ì„ ì°©ìˆœ ë°©ë¬¸ ì´ë²¤íŠ¸ ì§„í–‰ ì¤‘\nâ˜ï¸ ë¬¸ì˜: 010-0000-0000"
            ],
            "channel_talk_samples": [
                f"ğŸ”¥ {field_name} | íŒŒê²© ì¡°ê±´ë³€ê²½ ì†Œì‹!\nâœ… í•µì‹¬ í˜œíƒ ìš”ì•½:\n- ê³„ì•½ ì´ˆê¸° ìê¸ˆ ë¶€ë‹´ ì™„í™”\n- ì´ìå¿ƒé… ì—†ëŠ” {ib} í˜œíƒ\nğŸ“¢ ì”ì—¬ ì„¸ëŒ€ í™•ì¸ ğŸ‘‡",
                f"ğŸš¨ [ê¸´ê¸‰] {field_name} ë¡œì—´ì¸µ ì„ ì°©ìˆœ ë§ˆê° ì§ì „!\nğŸ’ íˆ¬ì/ì‹¤ê±°ì£¼ í¬ì¸íŠ¸:\n1. {address} ê¶Œì—­ ìµœìƒìœ„ ì…ì§€\n2. ì‹œì„¸ ì°¨ìµë§Œ {gap_percent}% ì´ìƒ ì˜ˆìƒ\nğŸ“ ê¸´ê¸‰ ìƒë‹´ ë¬¸ì˜: 010-0000-0000",
                f"ğŸ“Š {field_name} ì „ìš© [íŒ©íŠ¸ ì²´í¬ ë¦¬í¬íŠ¸]\nâœ¨ ë¦¬í¬íŠ¸ ìˆ˜ë¡ ë‚´ìš©:\n- {address} ê¶Œì—­ ë¶„ì„\n- ì¸ê·¼ ëŒ€ë¹„ {gap_percent}% ì €ë ´í•œ ë¶„ì–‘ê°€\nâ–¶ ë¦¬í¬íŠ¸ ì‹ ì²­: [ìƒë‹´ì˜ˆì•½ì‹ ì²­]",
                f"ğŸ—ï¸ {address}ì˜ íŒë„ë¥¼ ë°”ê¿€ [{field_name}]\nğŸ’ ë¸Œëœë“œ í”„ë¦¬ë¯¸ì—„ê³¼ ì••ë„ì  ì…ì§€\nğŸŒŸ ëœë“œë§ˆí¬ê°€ ë  ì´ìœ , ì§€ê¸ˆ í™•ì¸í•˜ì„¸ìš”.",
                f"ğŸ [{field_name}] íŠ¹ë³„ ë°©ë¬¸ ì´ë²¤íŠ¸!\në°©ë¬¸ë§Œ í•´ë„ ì¦ì •ë˜ëŠ” íŠ¹ë³„í•œ í˜œíƒ\nì§€ê¸ˆ ë°”ë¡œ ì˜ˆì•½í•˜ê³  ë¡œì—´ì¸µ ì„ ì í•˜ì„¸ìš”. âœ¨"
            ],
            "media_mix": [
                {"media": "í˜¸ê°±ë…¸ë…¸ ì±„ë„í†¡", "feature": "í˜„ì¥ ì§‘ì¤‘ ê´€ì‹¬ì", "reason": "ì‹¤ì‹œê°„ ë°ì´í„° ê¸°ë°˜", "strategy_example": "ì…ì§€ ë¶„ì„ ë¦¬í¬íŠ¸ ì¤‘ì‹¬ ìƒë‹´ ìœ ë„"},
                {"media": "LMS(ë¬¸ì ë§ˆì¼€íŒ…)", "feature": "ë‹¤ì´ë ‰íŠ¸ ë„ë‹¬", "reason": "ë†’ì€ ì¸ì§€ ë° í™•ì¸ìœ¨", "strategy_example": "í˜œíƒ ê°•ì¡° ë° ë°©ë¬¸ ì˜ˆì•½ ìœ ë„"},
                {"media": "ë©”íƒ€/ì¸ìŠ¤íƒ€ ë¦¬ë“œê´‘ê³ ", "feature": "DB ìˆ˜ëŸ‰ ê·¹ëŒ€í™”", "reason": "ê´€ì‹¬ì‚¬ ê¸°ë°˜ ëŒ€ëŸ‰ ë…¸ì¶œ", "strategy_example": "í˜œíƒ ìœ„ì£¼ ì†Œì¬ í™œìš©"}
            ]
        }

@app.get("/import-csv")
async def import_csv_data():
    """CSV íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ import"""
    import csv
    
    csv_file = "sites_data.csv"
    if not os.path.exists(csv_file):
        return {"status": "error", "message": "CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    
    imported = 0
    updated = 0
    
    try:
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            
            with Session(engine) as session:
                for row in reader:
                    site_id = row['id']
                    existing = session.get(Site, site_id)
                    
                    if existing:
                        existing.name = row['name']
                        existing.address = row['address']
                        existing.brand = row['brand'] if row['brand'] else None
                        existing.category = row['category']
                        existing.price = float(row['price'])
                        existing.target_price = float(row['target_price'])
                        existing.supply = int(row['supply'])
                        existing.down_payment = row.get('down_payment', '10%')
                        existing.interest_benefit = row.get('interest_benefit', 'ì¤‘ë„ê¸ˆ ë¬´ì´ì')
                        existing.status = row['status'] if row['status'] else None
                        updated += 1
                    else:
                        session.add(Site(
                            id=site_id,
                            name=row['name'],
                            address=row['address'],
                            brand=row['brand'] if row['brand'] else None,
                            category=row['category'],
                            price=float(row['price']),
                            target_price=float(row['target_price']),
                            supply=int(row['supply']),
                            down_payment=row.get('down_payment', '10%'),
                            interest_benefit=row.get('interest_benefit', 'ì¤‘ë„ê¸ˆ ë¬´ì´ì'),
                            status=row['status'] if row['status'] else None
                        ))
                        imported += 1
                session.commit()
        return {"status": "success", "imported": imported, "updated": updated}
    except Exception as e:
        logger.error(f"CSV import error: {e}")
        return {"status": "error", "message": str(e)}

class LeadSubmitRequest(BaseModel):
    name: str
    phone: str
    rank: str
    site: str
    source: Optional[str] = "ì•Œ ìˆ˜ ì—†ìŒ"

@app.post("/submit-lead")
async def submit_lead(req: LeadSubmitRequest):
    """ëª¨ìˆ˜ ì‹ ì²­(ë¦¬ë“œ) ì œì¶œ API"""
    try:
        with Session(engine) as session:
            new_lead = Lead(
                name=req.name,
                phone=req.phone,
                rank=req.rank,
                site=req.site,
                source=req.source
            )
            session.add(new_lead)
            session.commit()
            logger.info(f"New lead submitted: {req.name} ({req.site})")
            
            # êµ¬ê¸€ ì‹œíŠ¸ ì—°ë™ (ì›¹í›… URLì´ ì„¤ì •ëœ ê²½ìš°)
            if GOOGLE_SHEET_WEBHOOK_URL:
                try:
                    # êµ¬ê¸€ ë§¤í¬ë¡œëŠ” ë¦¬ë””ë ‰ì…˜ì„ ì‚¬ìš©í•˜ë¯€ë¡œ follow_redirects=Trueê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.
                    async with httpx.AsyncClient(follow_redirects=True) as client:
                        payload = {
                            "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            "name": req.name,
                            "phone": req.phone,
                            "rank": req.rank,
                            "site": req.site,
                            "source": req.source
                        }
                        # ë°ì´í„°ê°€ í™•ì‹¤íˆ ì „ì†¡ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
                        response = await client.post(GOOGLE_SHEET_WEBHOOK_URL, json=payload, timeout=8.0)
                        logger.info(f"Google Sheet webhook triggered. Status: {response.status_code}")
                except Exception as ex:
                    logger.error(f"Google Sheet sync error: {ex}")

        return {"status": "success", "message": "Lead submitted successfully"}
    except Exception as e:
        logger.error(f"Lead submission error: {e}")
        raise HTTPException(status_code=500, detail="ë¦¬ë“œ ì œì¶œ ì¤‘ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
@app.get("/")
async def root():
    return {"message": "Bunyang AlphaGo API is running"}

if __name__ == "__main__":
    create_db_and_tables()
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run("main:app", host="0.0.0.0", port=port, reload=True)
